{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport cv2\nimport pickle\nimport numpy as np\nimport xml.etree.ElementTree as ET\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tensorflow.keras.utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BASE_DIR = os.getcwd()\nimage_dir = os.path.join(BASE_DIR, '../input/face-mask-detection', 'images')\nannot_dir = os.path.join(BASE_DIR, '../input/face-mask-detection', 'annotations')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label2category = {'without_mask': 0, 'with_mask': 1, 'mask_weared_incorrect': 2}\ncategory2label = {v: k for k, v in label2category.items()}\ndatas = []\n\nfor root, dirs, files in os.walk(annot_dir):\n    for file in files:\n        tree = ET.parse(os.path.join(root, file))\n        data = {'path': None, 'objects': []}\n        data['path'] = os.path.join(image_dir, tree.find('filename').text)\n        for obj in tree.findall('object'):\n            label = label2category[obj.find('name').text]\n            # top left co-ordinates\n            xmin = int(obj.find('bndbox/xmin').text)\n            ymin = int(obj.find('bndbox/ymin').text)\n            # bottom right co-ordinates\n            xmax = int(obj.find('bndbox/xmax').text)\n            ymax = int(obj.find('bndbox/ymax').text)\n            data['objects'].append([label, xmin, ymin, xmax, ymax])\n        datas.append(data)\n\nprint('Total images :', len(datas))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index = np.random.randint(0, len(datas))\nimg = cv2.imread(datas[index]['path'])\nfor (category, xmin, ymin, xmax, ymax) in datas[index]['objects']:\n    # Draw bounding boxes\n    cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (0, 255, 0), 1)\n    cv2.putText(img, str(category), (xmin+2, ymin-3), cv2.FONT_HERSHEY_SIMPLEX, 0.38, (255,255,255), 2)\n# Show image\nplt.figure(figsize=(10, 6))\nplt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['objects']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_size = (100, 100) # same size of the input\nX = []\nY = []\n\nfor data in datas:\n    img = cv2.imread(data['path'])\n    for (category, xmin, ymin, xmax, ymax) in data['objects']:\n        roi = img[ymin : ymax, xmin : xmax]\n        roi = cv2.resize(roi, (100, 100))\n        data = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n        target = to_categorical(category, num_classes=len(category2label))\n        X.append(data)\n        Y.append(target)\n        \nX = np.array(X)\nY = np.array(Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(cv2.cvtColor(X[2], cv2.COLOR_BGR2RGB))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.save('', X)\nnp.save('', Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\npre_trained_model = InceptionV3(include_top=False, weights='imagenet', input_shape=(100, 100, 3))\n\npre_trained_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nfor layer in pre_trained_model.layers[:15]:\n    layer.trainable = False\n\nfor layer in pre_trained_model.layers[15:]:\n    layer.trainable = True\n'''\nfor layer in pre_trained_model.layers:\n    layer.trainable = False\n    \nlast_layer = pre_trained_model.get_layer('mixed7')\nprint('Last layer output shape :', last_layer.output_shape)\nlast_output = last_layer.output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = tf.keras.layers.Flatten()(last_output)\nx = tf.keras.layers.Dense(1024, activation='relu')(x)\nx = tf.keras.layers.Dropout(0.3)(x)\n#The Final layer with 3 outputs for 3 categories\nx = tf.keras.layers.Dense(3, activation='softmax')(x)\n\nmodel = tf.keras.models.Model(inputs=pre_trained_model.input, outputs=x)\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nprint(X.shape, Y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X / 255.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    filepath='data/model-{epoch:03d}.ckpt',\n    save_weights_only=True,\n    monitor='val_acc',\n    mode='max',\n    save_best_only=True, \n    verbose=0)\n\nhistory = model.fit(X_train, \n                    Y_train, \n                    epochs=20, \n                    callbacks=[checkpoint], \n                    validation_split=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PLOT LOSS AND ACCURACY\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.figure(figsize=(15, 6))\nplt.subplot(121)\nplt.plot(acc, label='Training')\nplt.plot(val_acc, label='Validation')\nplt.title('Training and validation accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\n\nplt.subplot(122)\nplt.plot(loss, label='Training')\nplt.plot(val_loss, label='Validation')\nplt.title('Training and validation loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(X_test, Y_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred = np.argmax(model.predict(X_test), axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_test = np.argmax(Y_test, axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('/output/inceptionV3-model.h5')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(confusion_matrix(Y_test, Y_pred), annot=True, fmt='g', cmap=plt.cm.Blues)\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing Libraries\nimport time\nimport cv2\nimport pickle\nimport numpy as np\nfrom tensorflow.keras.models import load_model\n\n# Load trained model\n#model = load_model('data/inceptionV3-model.h5')\n\n# Necessary values\n##with open('data/category2label.pkl', 'rb') as pf:\n#    category2label = pickle.load(pf)\n\nimg_size = (100, 100)\ncolors = {0: (0, 0, 255), 1: (0, 255, 0), 2: (0, 255, 255)}\n\n# Importing cascade classifier for face-detection\nface_cascade = cv2.CascadeClassifier('../input/haarcascade_frontalface_default.xml')\n\n# Video from webcam\ncap = cv2.VideoCapture(0)\n\nstart_time = time.time()\nframe_count = 0\n\nwhile cap.isOpened():\n    ret, frame = cap.read()  \n    \n    if not ret:\n        break\n    \n    frame_count += 1     # for fps\n    frame = cv2.flip(frame, 1)       # Mirror the image\n    faces = face_cascade.detectMultiScale(frame, 1.3, 5)\n    for (x, y, w, h) in faces:\n        # Predict\n        roi =  frame[y : y+h, x : x+w]\n        data = cv2.resize(cv2.cvtColor(roi, cv2.COLOR_BGR2RGB), img_size)\n        data = data / 255.\n        data = data.reshape((1,) + data.shape)\n        scores = model.predict(data)\n        target = np.argmax(scores, axis=1)[0]\n        # Draw bounding boxes\n        cv2.rectangle(img=frame, pt1=(x, y), pt2=(x+w, y+h), color=colors[target], thickness=2)\n        text = \"{}: {:.2f}\".format(category2label[target], scores[0][target])\n        cv2.putText(frame, text, (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n\n    elapsed_time = time.time() - start_time\n    fps = frame_count / elapsed_time\n    cv2.putText(img=frame, text='FPS : ' + str(round(fps, 2)), org=(10, 20), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.5, color=(255,255,255), thickness=1)\n\n    # Show the frame\n    cv2.imshow('Face Mask Detection', frame)\n\n    if cv2.waitKey(1) & 0xFF == 27:\n        break\n\ncap.release()\n#cv2.destroyAllWindows()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = load_model('output/inceptionV3-model.h5')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.chdir('kaggle/working')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'inceptionV3-model.h5')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}